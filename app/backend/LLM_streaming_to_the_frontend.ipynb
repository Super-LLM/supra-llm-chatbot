{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OBNXq4OuX7EU"
   },
   "source": [
    "Followed the advice of [llama-recipes](https://github.com/facebookresearch/llama-recipes/tree/main/demo_apps) , [TheBloke Llama 7B](https://huggingface.co/TheBloke/CodeLlama-7B-GGUF), [FastAPI project](https://github.com/pinecone-io/examples/blob/master/learn/generation/langchain/handbook/09-langchain-streaming/main.py) and [Langchain](https://python.langchain.com/docs/integrations/llms/llamacpp) on implementing this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Ap3JTNX6YEtZ"
   },
   "outputs": [],
   "source": [
    "# @title Set os environment keys\n",
    "import os\n",
    "os.environ[\"NGROK_AUTH_TOKEN\"] = # Add your authentication token here ðŸ˜…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ca_G3WCe0wxy",
    "outputId": "914680ca-7972-416d-d070-560410800ebf"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001B[2K     \u001B[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m \u001B[32m60.8/60.8 kB\u001B[0m \u001B[31m1.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K     \u001B[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m \u001B[32m1.8/1.8 MB\u001B[0m \u001B[31m27.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K     \u001B[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m \u001B[32m226.7/226.7 kB\u001B[0m \u001B[31m21.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K     \u001B[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m \u001B[32m52.0/52.0 kB\u001B[0m \u001B[31m6.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K     \u001B[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m \u001B[32m16.1/16.1 MB\u001B[0m \u001B[31m43.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K     \u001B[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m \u001B[32m79.9/79.9 MB\u001B[0m \u001B[31m4.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K     \u001B[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m \u001B[32m92.1/92.1 kB\u001B[0m \u001B[31m4.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K     \u001B[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m \u001B[32m58.3/58.3 kB\u001B[0m \u001B[31m3.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K     \u001B[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m \u001B[32m75.6/75.6 kB\u001B[0m \u001B[31m6.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K     \u001B[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m \u001B[32m3.1/3.1 MB\u001B[0m \u001B[31m34.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K     \u001B[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m \u001B[32m71.5/71.5 kB\u001B[0m \u001B[31m7.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K     \u001B[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m \u001B[32m77.8/77.8 kB\u001B[0m \u001B[31m6.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K     \u001B[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m \u001B[32m817.0/817.0 kB\u001B[0m \u001B[31m10.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K     \u001B[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m \u001B[32m1.7/1.7 MB\u001B[0m \u001B[31m11.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K     \u001B[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m \u001B[32m246.4/246.4 kB\u001B[0m \u001B[31m24.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K     \u001B[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m \u001B[32m62.1/62.1 kB\u001B[0m \u001B[31m7.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K     \u001B[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m \u001B[32m49.4/49.4 kB\u001B[0m \u001B[31m5.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K     \u001B[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m \u001B[32m138.5/138.5 kB\u001B[0m \u001B[31m15.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K     \u001B[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m \u001B[32m36.7/36.7 MB\u001B[0m \u001B[31m11.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25h  Installing build dependencies ... \u001B[?25l\u001B[?25hdone\n",
      "  Getting requirements to build wheel ... \u001B[?25l\u001B[?25hdone\n",
      "  Installing backend dependencies ... \u001B[?25l\u001B[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001B[?25l\u001B[?25hdone\n",
      "\u001B[2K     \u001B[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001B[0m \u001B[32m45.5/45.5 kB\u001B[0m \u001B[31m5.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25h  Building wheel for llama-cpp-python (pyproject.toml) ... \u001B[?25l\u001B[?25hdone\n",
      "Requirement already satisfied: llama-cpp-python in /usr/local/lib/python3.10/dist-packages (0.2.50)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (4.9.0)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (1.25.2)\n",
      "Requirement already satisfied: diskcache>=5.6.1 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (5.6.3)\n",
      "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.11.3->llama-cpp-python) (2.1.5)\n",
      "deb https://ngrok-agent.s3.amazonaws.com buster main\n",
      "Hit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
      "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n",
      "Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n",
      "Get:4 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
      "Hit:5 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
      "Get:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
      "Get:7 https://ngrok-agent.s3.amazonaws.com buster InRelease [20.3 kB]\n",
      "Hit:8 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
      "Get:9 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [1,859 kB]\n",
      "Hit:10 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
      "Get:11 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1,502 kB]\n",
      "Hit:12 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
      "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [1,907 kB]\n",
      "Hit:14 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
      "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,343 kB]\n",
      "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [1,786 kB]\n",
      "Get:17 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [713 kB]\n",
      "Get:18 https://ngrok-agent.s3.amazonaws.com buster/main amd64 Packages [3,730 B]\n",
      "Fetched 9,369 kB in 2s (5,656 kB/s)\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "39 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "The following NEW packages will be installed:\n",
      "  ngrok\n",
      "0 upgraded, 1 newly installed, 0 to remove and 39 not upgraded.\n",
      "Need to get 6,460 kB of archives.\n",
      "After this operation, 0 B of additional disk space will be used.\n",
      "Get:1 https://ngrok-agent.s3.amazonaws.com buster/main amd64 ngrok amd64 3.6.0 [6,460 kB]\n",
      "Fetched 6,460 kB in 1s (7,658 kB/s)\n",
      "debconf: unable to initialize frontend: Dialog\n",
      "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 1.)\n",
      "debconf: falling back to frontend: Readline\n",
      "debconf: unable to initialize frontend: Readline\n",
      "debconf: (This frontend requires a controlling tty.)\n",
      "debconf: falling back to frontend: Teletype\n",
      "dpkg-preconfigure: unable to re-open stdin: \n",
      "Selecting previously unselected package ngrok.\n",
      "(Reading database ... 121749 files and directories currently installed.)\n",
      "Preparing to unpack .../archives/ngrok_3.6.0_amd64.deb ...\n",
      "Unpacking ngrok (3.6.0) ...\n",
      "Setting up ngrok (3.6.0) ...\n"
     ]
    }
   ],
   "source": [
    "# @title Install depedencies\n",
    "\n",
    "# These exist just to make llama-cpp-python install without any errors\n",
    "%pip install uvicorn tiktoken openai cohere unicorn python-multipart kaleido fastapi -q\n",
    "\n",
    "# For downloading the model form huggingface\n",
    "%pip install huggingface_hub -q\n",
    "\n",
    "# For querying framework\n",
    "%pip install langchain langchainhub -q\n",
    "\n",
    "\n",
    "# More models in the \"Provided Files\" section of https://huggingface.co/TheBloke/CodeLlama-7B-GGUF.\n",
    "\n",
    "\n",
    "# @markdown Choose either GPU or CPU installation. You should most of the time choose GPU because it is much faster.\n",
    "computing_type = \"cpu\" # @param [\"gpu\", \"cpu\"] {type:\"string\"}\n",
    "if computing_type == \"cpu\":\n",
    "    # CPU INSTALLATION\n",
    "    %pip install --upgrade --quiet  llama-cpp-python\n",
    "    !CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python\n",
    "\n",
    "    # Reinstall cpu instead\n",
    "    # !CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install --upgrade --force-reinstall llama-cpp-python --no-cache-dir\n",
    "\n",
    "elif computing_type == \"gpu\":\n",
    "    # GPU INSTALLATION\n",
    "    !CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python\n",
    "\n",
    "\n",
    "#ngrok\n",
    "!curl -s https://ngrok-agent.s3.amazonaws.com/ngrok.asc | sudo tee /etc/apt/trusted.gpg.d/ngrok.asc >/dev/null && echo \"deb https://ngrok-agent.s3.amazonaws.com buster main\" | sudo tee /etc/apt/sources.list.d/ngrok.list && sudo apt update && sudo apt install ngrok\n",
    "\n",
    "# For the framework\n",
    "%pip install FastAPI pyngrok -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "form",
    "id": "BMPoSosU1JpD"
   },
   "outputs": [],
   "source": [
    "# @title Select the LLM model\n",
    "# @markdown Select a suitable model:\n",
    "model_name = \"llama-2-7b-chat.Q4_0.gguf\" # @param [\"llama-2-13b-chat.Q5_K_M.gguf\", \"llama-2-7b-chat.Q4_0.gguf\"] {type:\"string\"}\n",
    "\n",
    "\n",
    "if model_name == \"llama-2-13b-chat.Q5_K_M.gguf\":\n",
    "  # 13B model\n",
    "  huggingface_repository = \"TheBloke/Llama-2-13B-chat-GGUF\"\n",
    "elif model_name == \"llama-2-7b-chat.Q4_0.gguf\":\n",
    "  # 7B model\n",
    "  huggingface_repository = \"TheBloke/Llama-2-7b-Chat-GGUF\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zl8xfjNg1wpU",
    "outputId": "737058db-72a7-45e1-88ad-f4849e8885bb"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Consider using `hf_transfer` for faster downloads. This solution comes with some limitations. See https://huggingface.co/docs/huggingface_hub/hf_transfer for more details.\n",
      "downloading https://huggingface.co/TheBloke/Llama-2-7b-Chat-GGUF/resolve/main/llama-2-7b-chat.Q4_0.gguf to /root/.cache/huggingface/hub/tmpmb17yo5r\n",
      "llama-2-7b-chat.Q4_0.gguf: 100% 3.83G/3.83G [01:05<00:00, 58.8MB/s]\n",
      "./llama-2-7b-chat.Q4_0.gguf\n"
     ]
    }
   ],
   "source": [
    "# @title Download the LLM model\n",
    "!huggingface-cli download {huggingface_repository} {model_name} --local-dir . --local-dir-use-symlinks False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6UtiqUR3YqAR"
   },
   "source": [
    "Next we will setup the LLM and the fastapi service to be hosted on ngrok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "8WSqVYn1atGx"
   },
   "outputs": [],
   "source": [
    "# Lanchain LLM libraries\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.schema.output import LLMResult\n",
    "from langchain.callbacks.streaming_aiter import AsyncIteratorCallbackHandler\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.llms import LlamaCpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "GoM3Y38mzG7G"
   },
   "outputs": [],
   "source": [
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "model_path=\"/content/\" + model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RyjJMIv6zJRU",
    "outputId": "39b3c3a3-7e72-4337-e4b2-21f2d35d2cb7"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from /content/llama-2-7b-chat.Q4_0.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_0:  225 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_0\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 3.56 GiB (4.54 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors:        CPU buffer size =  3647.87 MiB\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 2048\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =  1024.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB\n",
      "llama_new_context_with_model:        CPU input buffer size   =     0.20 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =     2.50 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.context_length': '4096', 'general.name': 'LLaMA v2', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '11008', 'llama.attention.layer_norm_rms_epsilon': '0.000001', 'llama.rope.dimension_count': '128', 'llama.attention.head_count': '32', 'tokenizer.ggml.bos_token_id': '1', 'llama.block_count': '32', 'llama.attention.head_count_kv': '32', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '2'}\n"
     ]
    }
   ],
   "source": [
    "# Code from https://python.langchain.com/docs/integrations/llms/llamacpp\n",
    "if computing_type == \"cpu\":\n",
    "    # CPU INSTALLATION\n",
    "\n",
    "    llm = LlamaCpp(\n",
    "        model_path=\"/content/\" + model_name,\n",
    "        temperature=0.75,\n",
    "        max_tokens=2000,\n",
    "        n_ctx=2048,\n",
    "        top_p=1,\n",
    "        callback_manager=callback_manager,\n",
    "        streaming=True,\n",
    "        verbose=True,  # Verbose is required to pass to the callback manager\n",
    "    )\n",
    "elif computing_type == \"gpu\":\n",
    "    # Callbacks support token-wise streaming\n",
    "\n",
    "    n_gpu_layers = -1   # Change this value based on your model and your GPU VRAM pool.\n",
    "    n_batch = 512  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
    "\n",
    "    # Make sure the model path is correct for your system!\n",
    "    llm = LlamaCpp(\n",
    "        model_path=\"/content/\" + model_name,\n",
    "        temperature=0.75,\n",
    "        max_tokens=2000,\n",
    "        n_ctx=4096,\n",
    "        n_gpu_layers=n_gpu_layers,\n",
    "        n_batch=n_batch,\n",
    "        callback_manager=callback_manager,\n",
    "        verbose=True,  # Verbose is required to pass to the callback manager\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nAnA34yw4deX"
   },
   "outputs": [],
   "source": [
    "# @title Create template and test model\n",
    "test_response = False # @param [\"True\", \"False\"] {type:\"raw\"}\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's work this out in a step by step way to be sure we have the right answer.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "\n",
    "if test_response:\n",
    "  llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "  question = \"What NFL team won the Super Bowl in the year Justin Bieber was born?\"\n",
    "  response = llm.invoke(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CATGen3Oy6W6"
   },
   "source": [
    "Note: The agent type model is still somewhat broken, which is likely due to the too small LLM size. This is why it is recommended to select llm instead of agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "kbdmNltywuk5"
   },
   "outputs": [],
   "source": [
    "# @title Select model type for streaming.\n",
    "model_type = \"llm\" # @param [\"agent\", \"llm\"] {type:\"string\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QHHpreNUZRhi"
   },
   "outputs": [],
   "source": [
    "# @title Load the API model into memory.\n",
    "\n",
    "# Agent code from https://python.langchain.com/docs/modules/agents/how_to/handle_parsing_errors\n",
    "from enum import Enum\n",
    "import asyncio\n",
    "from typing import Annotated, List, Union, Dict, Any\n",
    "from fastapi import FastAPI, Query, Body, HTTPException\n",
    "from fastapi.responses import StreamingResponse\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from contextlib import asynccontextmanager\n",
    "from pydantic import BaseModel\n",
    "\n",
    "import datetime\n",
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "from contextlib import asynccontextmanager\n",
    "from langchain.agents import AgentType, initialize_agent, create_structured_chat_agent, AgentExecutor, create_react_agent\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain import hub\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Add CORS\n",
    "origin = ['*']\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=origin,\n",
    "    allow_credentials=True,\n",
    "    allow_methods=['*'],\n",
    "    allow_headers=['*']\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Message(BaseModel):\n",
    "    content: str\n",
    "\n",
    "if model_type == \"llm\":\n",
    "  class AsyncCallbackHandler(AsyncIteratorCallbackHandler):\n",
    "    content: str = \"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "      super().__init__()\n",
    "\n",
    "    async def on_llm_new_token(self, token: str, **kwargs: Any) -> None:\n",
    "      self.content += token\n",
    "      self.queue.put_nowait(token)\n",
    "\n",
    "    async def on_llm_end(self, response: LLMResult, **kwargs: Any) -> None:\n",
    "      self.content = \"\"\n",
    "      self.done.set()\n",
    "\n",
    "  async def runCall(query: str, stream_it: AsyncCallbackHandler):\n",
    "    llm.callbacks =[stream_it]\n",
    "    response = await llm.ainvoke(query)\n",
    "\n",
    "  async def createGen(query: str, stream_it: AsyncCallbackHandler):\n",
    "    task = asyncio.create_task(runCall(query, stream_it))\n",
    "    async for token in stream_it.aiter():\n",
    "        yield token\n",
    "    await task\n",
    "\n",
    "elif model_type == \"agent\":\n",
    "  memory = ConversationBufferWindowMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    k=5,\n",
    "    return_messages=True,\n",
    "    output_key=\"output\"\n",
    "  )\n",
    "  prompt = hub.pull(\"hwchase17/structured-chat-agent\")\n",
    "  # api_wrapper = WikipediaAPIWrapper(top_k_results=1, doc_content_chars_max=100)\n",
    "  # tool = WikipediaQueryRun(api_wrapper=api_wrapper)\n",
    "  # tools = [get_items, where_cat_is_hiding]\n",
    "  # agent = create_react_agent(llm, tools, prompt)\n",
    "  # agent_executor = AgentExecutor(\n",
    "  #     agent=agent, tools=tools, verbose=True, handle_parsing_errors=True\n",
    "  # ).with_config({\"run_name\": \"Agent\"})\n",
    "\n",
    "  # tools = ... # ADD TOOLS HERE\n",
    "  # create_structured_chat_agent()\n",
    "  # agent = create_structured_chat_agent(llm=llm, tools=tools, prompt=prompt)\n",
    "  # agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True, early_stopping_method=\"generate\",\n",
    "  #                                memory=memory, return_intermediate_steps=False)\n",
    "\n",
    "  # TODO: Replace with a non depricated tool. The code above would be a start as long as you replace tools.\n",
    "  agent = initialize_agent(\n",
    "      agent=AgentType.CHAT_CONVERSATIONAL_REACT_DESCRIPTION,\n",
    "      tools=[],\n",
    "      llm=llm,\n",
    "      verbose=True,\n",
    "      max_iterations=3,\n",
    "      early_stopping_method=\"generate\",\n",
    "      memory=memory,\n",
    "      return_intermediate_steps=False,\n",
    "  )\n",
    "\n",
    "  class AsyncCallbackHandler(AsyncIteratorCallbackHandler):\n",
    "    content: str = \"\"\n",
    "    final_answer: bool = False\n",
    "    def __init__(self) -> None:\n",
    "      super().__init__()\n",
    "\n",
    "    async def on_llm_new_token(self, token: str, **kwargs: Any) -> None:\n",
    "      self.content += token\n",
    "      # if we passed the final answer, we put tokens in queue\n",
    "      self.queue.put_nowait(token)\n",
    "      if self.final_answer:\n",
    "          if '\"action_input\": \"' in self.content:\n",
    "              if token not in ['\"', \"}\"]:\n",
    "                  print(f\"Token: {token}\")\n",
    "                  # self.queue.put_nowait(token)\n",
    "      elif \"Final Answer\" in self.content:\n",
    "          # print(\"----Final answer!!-----\")\n",
    "          self.final_answer = True\n",
    "          self.content = \"\"\n",
    "\n",
    "    async def on_llm_end(self, response: LLMResult, **kwargs: Any) -> None:\n",
    "      # print(\"\\n\\n\")\n",
    "      # print(\"------------------------------------\")\n",
    "      # print(\"END CONTENT PRINT OF ALL TOKENS:\")\n",
    "      # print(\"------------------------------------\")\n",
    "      # print(self.content)\n",
    "      # print(\"------------------------------------\")\n",
    "      # print(\"\\n\\n\")\n",
    "      if self.final_answer:\n",
    "            self.content = \"\"\n",
    "            self.final_answer = False\n",
    "            self.done.set()\n",
    "      else:\n",
    "          self.content = \"\"\n",
    "\n",
    "  async def runCall(query: str, stream_it: AsyncCallbackHandler):\n",
    "    agent.agent.llm_chain.llm.callbacks =[stream_it]\n",
    "    # response = await agent_executor.acall(inputs={\"input\": query})\n",
    "    response = await agent.acall(inputs={\"input\": query})\n",
    "\n",
    "  async def createGen(query: str, stream_it: AsyncCallbackHandler):\n",
    "    task = asyncio.create_task(runCall(query, stream_it))\n",
    "    async for token in stream_it.aiter():\n",
    "        yield token\n",
    "    await task\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@app.post(\"/query/\")\n",
    "async def get_response(time: datetime.datetime,\n",
    "                      query: Message = ...):\n",
    "  stream_it = AsyncCallbackHandler()\n",
    "  gen = createGen(query.content, stream_it)\n",
    "  return StreamingResponse(gen, media_type=\"text/event-stream\")\n",
    "\n",
    "\n",
    "@app.get(\"/health\")\n",
    "async def get_health():\n",
    "  return {\"Still here :)\"}\n",
    "\n",
    "@asynccontextmanager\n",
    "async def lifespan(app: FastAPI):\n",
    "   port = app.port\n",
    "   print(\"The port used for this app is\", port)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sPkLeWhdCWoV",
    "outputId": "9de6a323-e1b4-4be5-8d84-ec183fe40eea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
     ]
    }
   ],
   "source": [
    "# Put your ngrok authentication token here https://dashboard.ngrok.com/get-started/your-authtoken\n",
    "!ngrok config add-authtoken {os.environ[\"NGROK_AUTH_TOKEN\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "uylimNIDDhMO",
    "outputId": "14538411-2d63-410e-c0f9-a72f3b4e8ad6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [5954]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Public URL: https://fbbd-34-106-109-56.ngrok-free.app\n",
      "INFO:     185.76.9.58:0 - \"POST /query/?time=2024-02-24T16%3A33%3A17.093021 HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    5462.46 ms\n",
      "llama_print_timings:      sample time =     139.11 ms /   206 runs   (    0.68 ms per token,  1480.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5462.36 ms /     7 tokens (  780.34 ms per token,     1.28 tokens per second)\n",
      "llama_print_timings:        eval time =  170568.18 ms /   205 runs   (  832.04 ms per token,     1.20 tokens per second)\n",
      "llama_print_timings:       total time =  177129.07 ms /   212 tokens\n"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "from pyngrok import ngrok\n",
    "# import ngrok\n",
    "import uvicorn\n",
    "\n",
    "ngrok_tunnel = ngrok.connect(8000)\n",
    "print('Public URL:', ngrok_tunnel.public_url)\n",
    "nest_asyncio.apply()\n",
    "uvicorn.run(app, port=8000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2ERyY3NnL3f_"
   },
   "outputs": [],
   "source": [
    "# Use this to kill rogue ngrok instances\n",
    "# !killall ngrok"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
